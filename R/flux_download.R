
#' Download FLUXNET zip files
#'
#' Downloads zip files (one per site) for available FLUXNET sites.
#'
#' @param file_list_df When `NULL` (default), [flux_listall()] is used to
#'   determine the sites with data available to download (specific sites can be
#'   selected with `site_ids`).  If `file_list_df` is supplied, it should be a
#'   data frame generated by [flux_listall()], but potentially filtered to
#'   exlude some rows. This provides an alternative way of downloading only
#'   specific sites. See the examples for a possible use case. If `file_list_df`
#'   is not `NULL`, `cache_dir`, `use_cache`, and `cache_age` will be ingored
#'   but `site_ids` other than `"all"` will still be used.
#' @param site_ids Character; either `"all"` to download all sites available, or
#'   a vector of site IDs. For example, `c("UK-GaB", "CA-Ca2")`.
#' @param download_dir The directory to download zip files to.
#' @param overwrite Logical; overwrite already downloaded .zip files? If `FALSE`
#'   it will skip downloading existing files, unless they are invalid .zip files
#'   (e.g. due to partial download or corruption).
#' @inheritParams flux_listall
#'
#' @returns Invisibly returns the output of [curl::multi_download()], which
#'   contains information on download success, download time, HTTP errors, etc.
#' @examples
#' \dontrun{
#' # Download data for all available sites
#' flux_download()
#'
#' # Download data for just select site IDs
#' flux_download(site_ids = c("UK-GaB", "CA-Ca2"))
#'
#' # Download specific sites filtered by site metadata (IGPB  and data hub for example)
#' available_sites <- flux_listall()
#' to_get <-
#'   available_sites[available_sites$igbp == "CRO" & available_sites$data_hub == "AmeriFlux", ]
#' flux_download(file_list_df = to_get)
#'
#' # Get a fresh list of available data and overwrite any existing downloads
#' flux_download(use_cache = FALSE, overwrite = FALSE)
#' }
#'
#' @export
flux_download <- function(
  file_list_df = NULL,
  site_ids = "all",
  download_dir = "fluxnet",
  overwrite = FALSE,
  use_cache = TRUE,
  cache_dir = rappdirs::user_cache_dir("fluxnet"),
  cache_age = as.difftime(30, units = "days"),
  log_file = NULL,
  echo_cmd = FALSE
) {
  if (!is.null(file_list_df)) {
    if (!is.data.frame(file_list_df)) {
      cli::cli_abort(
        "{.var file_list_df} must be of class {.cls data.frame}, not {.cls {class(file_list_df)}}!"
      )
    }
  } else {
    file_list_df <- flux_listall(
      cache_dir = cache_dir,
      use_cache = use_cache,
      cache_age = cache_age
    )
  }
  if (length(site_ids) > 1 & !any(site_ids == "all")) {
    file_list_df <- file_list_df %>% dplyr::filter(.data$site_id %in% site_ids)
  }

  fs::dir_create(download_dir)

  # Check for existing files unless overwrite = TRUE
  if (isFALSE(overwrite)) {
    existing_paths <- fs::dir_ls(download_dir, glob = "*.zip")
    existing_files <- existing_paths %>% fs::path_file()

    existing_valid_files <- existing_files[purrr::map_lgl(
      existing_paths,
      check_zip
    )]

    file_list_df <- file_list_df %>%
      dplyr::filter(!.data$fluxnet_product_name %in% existing_valid_files)
  }
  # check that there are rows left after filtering
  if (nrow(file_list_df) == 0) {
    cli::cli_abort(
      "No files to download! Check that {.arg site_ids} are correct or that files aren't already downloaded if {.arg overwrite = FALSE}."
    )
  }
  resp <- curl::multi_download(
    urls = file_list_df$download_link,
    destfiles = fs::path(download_dir, file_list_df$fluxnet_product_name),
    resume = TRUE,
    useragent = "fluxnet R package (https://github.com/EcosystemEcologyLab/fluxnet-package)"
  )

  # Failed or interrupted
  failed <- resp |> dplyr::filter(.data$success == FALSE | is.na(.data$success))
  if (nrow(failed) > 0) {
    cli::cli_inform(
      "Retrying {nrow(failed)} failed downloads{?s}."
    )
    # Retry failed downloads once
    resp2 <- curl::multi_download(
      urls = failed$url,
      destfiles = failed$destfile,
      resume = TRUE,
      useragent = "fluxnet R package (https://github.com/EcosystemEcologyLab/fluxnet-package)"
    )
    resp <- dplyr::bind_rows(
      resp %>% dplyr::filter(.data$success == TRUE),
      resp2
    )
  }
  # Failed or interrupted
  failed2 <- resp %>%
    dplyr::filter(.data$success == FALSE | is.na(.data$success))
  # If there are still failures, print a warning
  if (nrow(failed2) > 0) {
    failed_sites <- dplyr::left_join(
      failed2 %>%
        dplyr::mutate(fluxnet_product_name = fs::path_file(.data$destfile)),
      file_list_df,
      by = "fluxnet_product_name"
    ) %>%
      dplyr::pull("site_id")
    failed_sites_formatted <- glue::glue('"{failed_sites}"') %>%
      glue::glue_collapse(", ")

    cli::cli_warn(c(
      "Incomplete downloads for {length(failed_sites)} site{?s}",
      "i" = "Run {.run flux_download(site_ids = c({failed_sites_formatted}))} to try again."
    ))
  }
  return(invisible(resp))
}


check_zip <- function(zip_file) {
  test <- try(utils::unzip(zip_file, list = TRUE), silent = TRUE)
  if (inherits(test, "try-error")) {
    return(FALSE)
  } else {
    return(TRUE)
  }
}
