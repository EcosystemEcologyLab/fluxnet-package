#' List available FLUXNET zip files for download
#'
#' This provides a wrapper around the
#' [fluxnet-shuttle](https://github.com/fluxnet/shuttle) command-line utility's
#' `listall` command, which downloads a data frame of available .zip files. By
#' default, the downloaded CSV is stored in
#' `rappdirs::user_cache_dir("fluxnet")`.  If there is allready a FLUXNET
#' shanpshot CSV file downloaded and it is more recent than `cache_age`, it will
#' be read in instead of downloading a new snapshot (unless `cache = FALSE`).
#'
#' @param cache_dir The directory to store the list of available FLUXNET data
#'   in.
#' @param use_cache Logical; use cached list of files available to download if
#'   it exists and is not older than `cache_age`?
#' @param cache_age A `difftime` object of length 1. If there are no cached
#'   snapshots more recent than `cache_age`, a new one will be downloaded and
#'   stored. You can force the cache to be invalidated with `cache_age = -Inf`.
#' @param log_file An optional file path (e.g. `"log.txt"`) to direct the
#'   `fluxnet-shuttle` log to. Useful for debugging.
#' @param echo_cmd Set to `TRUE` to print the shell command in the console.
#'   Passed to [processx::run()].
#' @returns A data frame of stations with available data and their metadata.
#' @examples
#' \dontrun{
#' fluxnet_files <- flux_listall()
#' }
#' @export
flux_listall <- function(
  cache_dir = rappdirs::user_cache_dir("fluxnet"),
  use_cache = TRUE,
  cache_age = as.difftime(30, units = "days"),
  log_file = NULL,
  echo_cmd = FALSE
) {
  # Check if there is already a recently downloaded list
  fs::dir_create(cache_dir)
  cached_snapshots <- dplyr::tibble(
    path = fs::dir_ls(
      cache_dir,
      regexp = "fluxnet_shuttle_snapshot_\\d+T\\d+\\.csv$"
    )
  ) |>
    dplyr::mutate(timestamp = stringr::str_extract(path, "\\d+T\\d+")) |>
    dplyr::mutate(datetime = lubridate::ymd_hms(timestamp)) |>
    dplyr::mutate(expired = datetime + cache_age < Sys.time()) |>
    dplyr::arrange(desc(datetime))

  if (
    nrow(cached_snapshots |> dplyr::filter(!expired)) == 0 |
      isFALSE(use_cache)
  ) {
    # # Check that fluxnet-shuttle is installed
    # shuttle_installed <- processx::run(
    #   "which",
    #   "fluxnet-shuttle",
    #   error_on_status = FALSE,
    #   stderr = NULL,
    #   stdout = NULL
    # )
    # if (shuttle_installed$status > 0) {
    #   cli::cli_abort(
    #     "To use {.fn flux_listall}, install the {.code fluxnet-shuttle} command-line utility at {.url https://github.com/fluxnet/shuttle}."
    #   )
    # }
    fluxnet_shuttle <- fluxnet_shuttle_executable("fluxnet")
    cli::cli_inform("File list is expired, downloading the latest version")
    # Run from cache_dir instead of supplying cache_dir to -o flag because -l flag to set path
    # of logfile doesn't work (https://github.com/fluxnet/shuttle/issues/104)
    if (is.null(log_file)) {
      log_cmd <- "--no-logfile"
    } else {
      log_cmd <- c("-l", log_file)
    }
    listall <- processx::run(
      fluxnet_shuttle,
      c(log_cmd, "listall", "-o", fs::path_expand(cache_dir)),
      echo_cmd = echo_cmd
    )
    csv_file <- listall$stdout |>
      stringr::str_extract("(?<=snapshot written to ).+")

    list <- readr::read_csv(
      fs::path(csv_file),
      show_col_types = FALSE
    )
    return(list)
  } else {
    #just read the newest cached one
    csv_path <- cached_snapshots |>
      dplyr::filter(!expired & datetime == max(datetime)) |>
      dplyr::pull(path)
    list <- readr::read_csv(csv_path, show_col_types = FALSE)
    return(list)
  }
}

#' Download FLUXNET zip files
#'
#' Downloads zip files (one per site) for available FLUXNET sites.
#'
#' @param file_list_df When `NULL` (default), [flux_listall()] is used to
#'   determine the sites with data available to download (specific sites can be
#'   selected with `site_ids`).  If `file_list_df` is supplied, it should be a
#'   data frame generated by [flux_listall()], but potentially filtered to
#'   exlude some rows. This provides an alternative way of downloading only
#'   specific sites. See the examples for a possible use case. If
#'   `file_list_df` is not `NULL`, `cache_dir`, `cache`, and `cache_age` will be
#'   ingored but `site_ids` other than `"all"` will still be used.
#' @param site_ids Character; either `"all"` to download all sites available, or
#'   a vector of site IDs. For example, `c("UK-GaB", "CA-Ca2")`.
#' @param download_dir The directory to download zip files to.
#' @param overwrite Logical; overwrite already downloaded .zip files?
#' @inheritParams flux_listall
#'
#' @returns Invisibly returns the output of [curl::multi_download()], which
#'   contains information on download success, download time, HTTP errors, etc.
#' @examples
#' \dontrun{
#' # Download data for all available sites
#' flux_download()
#'
#' # Download data for just select site IDs
#' flux_download(site_ids = c("UK-GaB", "CA-Ca2"))
#'
#' # Download specific sites filtered by site metadata (IGPB  and data hub for example)
#' available_sites <- flux_listall()
#' to_get <-
#'   available_sites[available_sites$igbp == "CRO" & available_sites$data_hub == "AmeriFlux", ]
#' flux_download(file_list_df = to_get)
#'
#' # Get a fresh list of available data and overwrite any existing downloads
#' flux_download(use_cache = FALSE, overwrite = FALSE)
#' }
#'
#' @export
flux_download <- function(
  file_list_df = NULL,
  site_ids = "all",
  download_dir = "fluxnet",
  overwrite = FALSE,
  use_cache = TRUE,
  cache_dir = rappdirs::user_cache_dir("fluxnet"),
  cache_age = as.difftime(30, units = "days"),
  log_file = NULL,
  echo_cmd = FALSE
) {
  if (!is.null(file_list_df)) {
    if (!is.data.frame(file_list_df)) {
      cli::cli_abort(
        "{.var file_list_df} must be of class {.cls data.frame}, not {.cls {class(file_list_df)}}!"
      )
    }
  } else {
    file_list_df <- flux_listall(
      cache_dir = cache_dir,
      use_cache = use_cache,
      cache_age = cache_age
    )
  }
  if (length(site_ids) > 1 & !any(site_ids == "all")) {
    file_list_df <- file_list_df |> dplyr::filter(site_id %in% site_ids)
  }

  # Check for existing files unless overwrite = TRUE
  fs::dir_create(download_dir)

  if (isFALSE(overwrite)) {
    existing_files <- fs::dir_ls(download_dir) |> fs::path_file()
    file_list_df <- file_list_df |>
      dplyr::filter(!fluxnet_product_name %in% existing_files)
  }
  # check that there are rows left after filtering
  if (nrow(file_list_df) == 0) {
    cli::cli_abort(
      "No files to download! Check that {.arg site_ids} are correct or that files aren't already downloaded if {.arg overwrite = FALSE}."
    )
  }
  resp <- curl::multi_download(
    urls = file_list_df$download_link,
    destfiles = fs::path(download_dir, file_list_df$fluxnet_product_name)
  )
  # TODO: add retry with resume for failed and partial downloads.
  return(invisible(resp))
}
